\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage[russian]{babel}
\usepackage{float}


\renewcommand{\thesection}{\arabic{section}.}

\title{Методы оптимизации, Лабораторная работа №3}
\author{Кирилл Кадомцев}
\date{Май 2025}

\begin{document}
\maketitle
\tableofcontents

\section{Описание}
Было реализовано решение задачи линейной регрессии с использованием стохастического (и не только) градиентного спуска. В качестве модификации, был реализован SGD с инерцией. Не могу не подметить, что реализованная в первой лабораторной поддержка градиентного спуска с разными размерностями приятно помогла - стратегии выбора шага были использованы оттуда.


\section{Данные и задача}
В качестве дата-сета использовались данные, содержащие результаты физико-химических тестов и оценку качества вина. Таким образом можно смоделировать связь этих тестов и величину оценки. Любопытно то, что качество - параметр субъективный, пусть и для получения этого параметра используется определённый алгоритм, завязан он на восприятии, в то время как результаты тестов - параметр вполне объективный

\section{Тестирование}
С сожалением признаем, что производительность устройства не позволила протестировать в полной мере - пришлось добавить жёсткую отсечку по количеству итераций, поэтому замеры не вполне объективны, поскольку отсекались после 1000 итераций. Кроме того, в связи с использованием \textit{numpy}, количество операций бралось усреднённое.

Тестировались сочетания следующих параметров:
\begin{enumerate}
    \item Размер батча
    \begin{itemize}
        \item 1 - стохастический градиентный спуск
        \item 32 - mini-batch градиентный спуск
        \item full - полный градиентный спуск
    \end{itemize}
    \item Стратегия выбора шага для градиентного спуска
    \begin{itemize}
        \item 
        Константная 
        \item 
        Адатптивно-константная
        \item 
        С инерцией (как часть модификации)
    \end{itemize}
    \item Методы регуляризации
    \begin{itemize}
        \item 
        Без регуляризации
        \item 
        L1-регуляризация
        \item 
        L2-регуляризация
        \item 
        elasticnet
    \end{itemize}
\end{enumerate}
Сравнивались, в свою очередь, следующие параметры:
\begin{enumerate}
    \item 
    Средне-квадратическая ошибка
    \item
    Время выполнения
    \item 
    Расход памяти
    \item 
    Количество арифметических операций (проблема см. выше)
\end{enumerate}

\section{Результаты тестирования}

\begin{table}[htbp]
\centering
\scriptsize
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\textbf{№} & \textbf{Strategy} & \textbf{Batch size} & \textbf{Regularization} & \textbf{MSE} & \textbf{Time (s)} & \textbf{Memory (MB)} & \textbf{Ops} \\
\hline
1 & Constant & 1 & none & 0.5352 & 109.08 & 0.01 & 76,752,000 \\
2 & Constant & 1 & l2 & 0.7885 & 121.92 & 0.01 & 76,752,000 \\
3 & Constant & 1 & l1 & 0.5199 & 124.82 & 0.01 & 76,752,000 \\
4 & Constant & 1 & elasticnet & 0.5361 & 155.33 & 0.01 & 76,752,000 \\
5 & Constant & 32 & none & 0.4169 & 3.38 & 0.02 & 76,752,000 \\
6 & Constant & 32 & l2 & 0.6791 & 3.98 & 0.02 & 76,752,000 \\
7 & Constant & 32 & l1 & 0.4357 & 4.05 & 0.02 & 76,752,000 \\
8 & Constant & 32 & elasticnet & 0.5100 & 4.91 & 0.02 & 76,752,000 \\
9 & Constant & full & none & 0.4168 & 0.08 & 0.16 & 76,752,000 \\
10 & Constant & full & l2 & 0.6809 & 0.09 & 0.16 & 76,752,000 \\
11 & Constant & full & l1 & 0.4360 & 0.09 & 0.16 & 76,752,000 \\
12 & Constant & full & elasticnet & 0.5096 & 0.11 & 0.16 & 76,752,000 \\
\hline
13 & Momentum & 1 & none & 0.4836 & 113.78 & 0.01 & 76,752,000 \\
14 & Momentum & 1 & l2 & 0.7359 & 134.51 & 0.01 & 76,752,000 \\
15 & Momentum & 1 & l1 & 0.4488 & 137.34 & 0.01 & 76,752,000 \\
16 & Momentum & 1 & elasticnet & 0.5271 & 166.69 & 0.01 & 76,752,000 \\
17 & Momentum & 32 & none & 0.4170 & 3.82 & 0.02 & 76,752,000 \\
18 & Momentum & 32 & l2 & 0.6791 & 4.39 & 0.02 & 76,752,000 \\
19 & Momentum & 32 & l1 & 0.4364 & 4.44 & 0.02 & 76,752,000 \\
20 & Momentum & 32 & elasticnet & 0.5110 & 5.31 & 0.02 & 76,752,000 \\
21 & Momentum & full & none & 0.4168 & 0.08 & 0.16 & 76,752,000 \\
22 & Momentum & full & l2 & 0.6809 & 0.09 & 0.16 & 76,752,000 \\
23 & Momentum & full & l1 & 0.4360 & 0.10 & 0.16 & 76,752,000 \\
24 & Momentum & full & elasticnet & 0.5096 & 0.12 & 0.16 & 76,752,000 \\
\hline
25 & Piecewise & 1 & none & 0.4776 & 97.88 & 0.03 & 58,706,304 \\
26 & Piecewise & 1 & l2 & 27.1317 & 148.51 & 0.01 & 76,752,000 \\
27 & Piecewise & 1 & l1 & $\infty$ & 154.26 & 0.01 & 76,752,000 \\
28 & Piecewise & 1 & elasticnet & 61.8512 & 189.13 & 0.01 & 76,752,000 \\
29 & Piecewise & 32 & none & 0.4259 & 4.19 & 0.02 & 76,752,000 \\
30 & Piecewise & 32 & l2 & 0.7200 & 5.70 & 0.02 & 76,752,000 \\
31 & Piecewise & 32 & l1 & 0.4415 & 5.55 & 0.02 & 76,752,000 \\
32 & Piecewise & 32 & elasticnet & 0.5282 & 6.59 & 0.02 & 76,752,000 \\
33 & Piecewise & full & none & 0.4168 & 0.09 & 0.16 & 76,752,000 \\
34 & Piecewise & full & l2 & 0.6809 & 0.11 & 0.16 & 76,752,000 \\
35 & Piecewise & full & l1 & 0.4361 & 0.11 & 0.16 & 76,752,000 \\
36 & Piecewise & full & elasticnet & 0.5096 & 0.12 & 0.16 & 76,752,000 \\
\hline
\end{tabular}
\caption{Результаты тестирования стратегий обучения SGD с различными параметрами}
\end{table}

\subsection{В зависимости от выбора размера batch}
\begin{enumerate}
    \item 
    Полный градиент даёт наибольшую скорость обучения (0.08–0.12 сек) и оптимальный MSE (0.42–0.51)
    \item
    Мини-батчи дают меньшую скорость обучения (3–6 сек), но немного лучший MSE (0.41–0.50), хотя и незначительно, что однако может быть связано с описанными выше проблемами
    \item
    Стохастический выдал значительно меньшую скорость обучения (до 3 минут), при этом MSE получался даже хуже
\end{enumerate}

Таким образом, стохастический здесь оказался бесполезен, а полный градиент - компромиссом между временем выполнения и MSE.

\subsection{В зависимости от регуляризации}
\begin{enumerate}
    \item 
    Наилучший MSE получился с L1-регуляризацией
    \item 
    Наилучшее время было при отсутствии регуляризации вовсе
    \item
    L2-регуляризация потратила наибольшее количество времени, при этом MSE был значительно хуже.
\end{enumerate}

Таким образом, оптимальный вариант - L1-регуляризация

\subsection{В зависимости от выбора шага}
\begin{enumerate}
    \item 
    Модификация - шаг с инерцией - оказалась выигрышной по MSE и незначительно уступила по времени константному шагу
    \item
    Адаптивная константа без регуляризации для стохастического градиентного спуска выдала MSE лучший, чем у константного выбора шага, однако с регуляризацией не отработала вовсе. Для размеров батчей MSE оказался в среднем между константой и инерцией.
\end{enumerate}

\section{Дополнительное задание №2}
В качестве рассматриваемой темы было выбрано определение жанра музыки по аудиофайлу на основе метода опорных векторов. Ниже опишем подход.

Обучающая выборка: \[
\mathcal{D} = \left\{(x_i, y_i)\right\}_{i=1}^n,\quad x_i \in R^d,\quad y_i \in \{-1, +1\} \text{ или } \{1, \dots, K\}
\]
Под $x_i$ рассматривается \textit{MFCC}, однако иногда используется также спектрограмма.

Коротко о методе \textit{MFCC}:
\begin{enumerate}
    \item 
    Сигнал разбивается на короткие окна (менее 1 секунды)
    \item 
    Вычисляется спектр сигнал
    \item
    Накладывается фильтр mel (единица высоты звука, отражающая восприятие человеком) \\
    $m = 2595 \cdot log_{10}\left(1 + \frac{f}{100}\right)$
    \item 
    Амплитуды логарифмируются
\end{enumerate}

Метод опорных векторов здесь ищет гиперплоскость, которая будет максимально разделять жанры. 
\end{document}